{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8a51eec-74df-4c53-a8a8-104b5532ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. 下载 Tatoeba 英法并只取前 1% 样本以加速实验\n",
    "raw = load_dataset(\"tatoeba\", lang1=\"en\", lang2=\"fr\", split=\"train[:1%]\")  # Tatoeba 英法平行语料 :contentReference[oaicite:4]{index=4}\n",
    "\n",
    "# 2. 初始化分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# 3. 预处理：添加翻译前缀，tokenize，并将 padding token 转为 -100 以忽略\n",
    "def preprocess(examples):\n",
    "    inputs  = [\"translate English to French: \" + ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    mi = tokenizer(inputs,  max_length=64, truncation=True, padding=\"max_length\")\n",
    "    lbl = tokenizer(targets, max_length=64, truncation=True, padding=\"max_length\").input_ids\n",
    "    lbl = [[(t if t != tokenizer.pad_token_id else -100) for t in seq] for seq in lbl]\n",
    "    mi[\"labels\"] = lbl\n",
    "    return mi\n",
    "\n",
    "tokenized = raw.map(preprocess, batched=True, remove_columns=[\"translation\"])  # 动态预处理 :contentReference[oaicite:5]{index=5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58b57ec6-156d-442d-8f7b-fe506609e6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 23:04:23.704870: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2025-04-23 23:04:23.704912: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 18.00 GB\n",
      "2025-04-23 23:04:23.704918: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 6.00 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745420663.704931  273416 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1745420663.704962  273416 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSeq2SeqLM\n",
    "\n",
    "# 可定制的超参数\n",
    "MODEL_CHECKPOINT = \"t5-small\"   # 可换成 t5-base、t5-large 等 :contentReference[oaicite:6]{index=6}\n",
    "LEARNING_RATE     = 5e-5\n",
    "NUM_BEAMS         = 4\n",
    "\n",
    "# 加载模型\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)  # 加载预训练 T5 编码器-解码器 :contentReference[oaicite:7]{index=7}\n",
    "\n",
    "# 若需调整层数 / 头数，可在此处重新定义 config\n",
    "# e.g., model.config.num_layers = 4; model.config.num_heads = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71eb64ae-9150-485b-9900-d942eab3c639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lordtarn1shed/anaconda3/envs/transformer/lib/python3.10/site-packages/datasets/arrow_dataset.py:400: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 23:04:30.159604: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745420670.638479  273416 meta_optimizer.cc:967] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/AssignAddVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/166 [==============================] - 70s 351ms/step - loss: 0.9602\n",
      "Epoch 2/2\n",
      "166/166 [==============================] - 58s 346ms/step - loss: 0.8748\n",
      "166/166 [==============================] - 27s 151ms/step - loss: 0.6645\n",
      "Test loss: 0.6644615530967712\n"
     ]
    }
   ],
   "source": [
    "# 转为 tf.data.Dataset\n",
    "tf_train = tokenized.to_tf_dataset(\n",
    "    columns=[\"input_ids\",\"attention_mask\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    ")\n",
    "tf_test = tokenized.to_tf_dataset(\n",
    "    columns=[\"input_ids\",\"attention_mask\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# 编译与训练\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer)\n",
    "model.fit(tf_train, epochs=2)                  # 训练 :contentReference[oaicite:8]{index=8}\n",
    "results = model.evaluate(tf_test)              # 评估\n",
    "print(\"Test loss:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1516bc39-bcef-4713-bc72-78b726e4c48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lordtarn1shed/anaconda3/envs/transformer/lib/python3.10/site-packages/transformers/generation/tf_utils.py:837: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "I0000 00:00:1745420820.138547  273416 service.cc:152] XLA service 0x600003454500 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1745420820.138564  273416 service.cc:160]   StreamExecutor device (0): Host, Default Version\n",
      "2025-04-23 23:07:00.159105: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1745420820.327738  273416 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: C'est un test.\n"
     ]
    }
   ],
   "source": [
    "# 推理示例\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "sample = \"translate English to French: This is a test.\"\n",
    "inp = tokenizer(sample, return_tensors=\"tf\", padding=\"max_length\", max_length=64)\n",
    "with tf.device('/CPU:0'):\n",
    "    out = model.generate(inp[\"input_ids\"], attention_mask=inp[\"attention_mask\"], num_beams=NUM_BEAMS)\n",
    "print(\"Translation:\", tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8bfe77-1d44-4f74-b881-9ae767b9dca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (transformer)",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
