{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a51eec-74df-4c53-a8a8-104b5532ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_dataset_builder\n",
    "from transformers import AutoTokenizer\n",
    "from src import tensorfi2 as tfi\n",
    "from transformers import MT5TokenizerFast\n",
    "\n",
    "\n",
    "builder = load_dataset_builder(\"IWSLT/iwslt2017\", name=\"iwslt2017-en-zh\")\n",
    "num_train_samples = builder.info.splits[\"train\"].num_examples\n",
    "print(f\"Total samples in IWSLT/iwslt2017 en-zh train split: {num_train_samples}\")\n",
    "# Calculate 0.1% of the samples\n",
    "samples_to_load = int(0.001 * num_train_samples)\n",
    "print(f\"Loading {samples_to_load} samples (0.1% of train split)\")\n",
    "# 1. 下载 wmt14 (英德对) 并只取 0.1% 样本以加速实验\n",
    "raw = load_dataset(\"IWSLT/iwslt2017\", name=\"iwslt2017-en-zh\", split=f\"train[:{samples_to_load}]\")\n",
    "\n",
    "# 2. 初始化分词器\n",
    "tokenizer = MT5TokenizerFast.from_pretrained(\"google/mt5-small\")\n",
    "\n",
    "# 3. 预处理：添加翻译前缀，tokenize，并将 padding token 转为 -100 以忽略\n",
    "def preprocess(examples):\n",
    "    inputs  = [\"translate English to Chinese: \" + ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"zh\"] for ex in examples[\"translation\"]]\n",
    "    mi = tokenizer(inputs,  max_length=64, truncation=True, padding=\"max_length\")\n",
    "    lbl = tokenizer(targets, max_length=64, truncation=True, padding=\"max_length\").input_ids\n",
    "    lbl = [[(t if t != tokenizer.pad_token_id else -100) for t in seq] for seq in lbl]\n",
    "    mi[\"labels\"] = lbl\n",
    "    return mi\n",
    "\n",
    "tokenized = raw.map(preprocess, batched=True, remove_columns=[\"translation\"])  # 动态预处理 :contentReference[oaicite:5]{index=5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b57ec6-156d-442d-8f7b-fe506609e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSeq2SeqLM\n",
    "from tensorflow.keras import mixed_precision\n",
    "#mixed_precision.set_global_policy('mixed_float16')  # GPU自动加速\n",
    "\n",
    "# 可定制的超参数\n",
    "MODEL_CHECKPOINT = \"google/mt5-small\"   # 可换成 t5-base、t5-large 等 :contentReference[oaicite:6]{index=6}\n",
    "LEARNING_RATE     = 5e-5\n",
    "NUM_BEAMS         = 4\n",
    "\n",
    "# 加载模型\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)  # 加载预训练 T5 编码器-解码器 :contentReference[oaicite:7]{index=7}\n",
    "\n",
    "# 若需调整层数 / 头数，可在此处重新定义 config\n",
    "# e.g., model.config.num_layers = 4; model.config.num_heads = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train =( \n",
    "    tokenized.to_tf_dataset(\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "        batch_size=16,\n",
    "        shuffle=True\n",
    "    )\n",
    "    .cache()                        # 缓存到内存/磁盘\n",
    "    .shuffle(2000, reshuffle_each_iteration=True)  # 增加buffer_size\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)  # 异步预取\n",
    "    .repeat()  # 避免每个epoch重新初始化\n",
    ")\n",
    "\n",
    "\n",
    "tf_test = tokenized.to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    batch_size=16,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edc5522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_generate(model, input_ids, attention_mask, **gen_kwargs):\n",
    "    # 1. 保存原始精度策略\n",
    "    orig_policy = mixed_precision.global_policy().name\n",
    "\n",
    "    try:\n",
    "        # 2. 切换到 float32，再执行 generate\n",
    "        mixed_precision.set_global_policy('float32')\n",
    "        return model.generate(input_ids, attention_mask=attention_mask, use_cache=False, **gen_kwargs)\n",
    "\n",
    "    except tf.errors.NotFoundError as e:\n",
    "        print(\"⚠️ generate 报 NotFoundError，切到 CPU 重试:\", e)\n",
    "        # 3. 在 CPU 上重试\n",
    "        with tf.device('/CPU:0'):\n",
    "            return model.generate(input_ids, attention_mask=attention_mask, use_cache=False, **gen_kwargs)\n",
    "\n",
    "    finally:\n",
    "        # 4. 恢复原始精度策略\n",
    "        mixed_precision.set_global_policy(orig_policy)\n",
    "\n",
    "def evaluate_robustness(model,\n",
    "                        tf_test,\n",
    "                        num_batches: int = 100,\n",
    "                        conf_file: str = \"conf/sample.yaml\"):\n",
    "    \"\"\"\n",
    "    Evaluate SDC rate and crash rate on a subset of tf_test.\n",
    "    \n",
    "    Args:\n",
    "      model      : 已加载的 TFAutoModelForSeq2SeqLM 实例\n",
    "      tf_test    : tf.data.Dataset，包含 input_ids, attention_mask, labels\n",
    "      num_batches: 取多少个 batch 进行测试\n",
    "      flip_prob  : 注入 bit‐flip 失败概率\n",
    "      conf_file  : TensorFI 的配置文件路径\n",
    "    \n",
    "    Returns:\n",
    "      sdc_rate   : (num_sdc / total_samples)\n",
    "      crash_rate : (num_crash / total_samples)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 先收集基线输出\n",
    "    baseline_outputs = []\n",
    "    input_batches = []\n",
    "    for i, batch in enumerate(tf_test):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        input_ids      = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        # 使用 beam search 生成结果\n",
    "        preds = model.generate(input_ids,\n",
    "                               attention_mask=attention_mask,\n",
    "                               num_beams=NUM_BEAMS,\n",
    "                               max_length=64,\n",
    "                               use_cache=False)\n",
    "        baseline_outputs.append(preds)\n",
    "        input_batches.append((input_ids, attention_mask))\n",
    "\n",
    "    total = len(baseline_outputs)\n",
    "    sdc_count   = 0\n",
    "    crash_count = 0\n",
    "\n",
    "    # 注入前保存\n",
    "    orig_weights = model.get_weights()  \n",
    "\n",
    "    # 2. 对每个 batch 注入故障并重推理\n",
    "    for (input_ids, attention_mask), base_preds in zip(input_batches, baseline_outputs):\n",
    "        try:\n",
    "            # 在模型上注入硬件故障\n",
    "            tfi.inject(model=model,\n",
    "                       confFile=conf_file,\n",
    "                       log_level=\"ERROR\")\n",
    "\n",
    "            # 带故障推理\n",
    "            faulty_preds = safe_generate(model,            # ← 这里一定要传 model\n",
    "                                        input_ids,\n",
    "                                        attention_mask=attention_mask,\n",
    "                                        num_beams=NUM_BEAMS,\n",
    "                                        max_length=64)\n",
    "\n",
    "\n",
    "            # 若推理结果与基线不同，则计作 SDC\n",
    "            if not tf.reduce_all(faulty_preds == base_preds):\n",
    "                sdc_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            # 推理过程中崩溃\n",
    "            crash_count += 1\n",
    "\n",
    "        finally:\n",
    "            # 恢复模型到无注入状态\n",
    "            # 恢复模型权重\n",
    "            model.set_weights(orig_weights)\n",
    "\n",
    "\n",
    "    sdc_rate   = sdc_count / total\n",
    "    crash_rate = crash_count / total\n",
    "    return sdc_rate, crash_rate, total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eb64ae-9150-485b-9900-d942eab3c639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 loss 函数和优化器\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "lambda_weight = 0  # 正则项权重，可调\n",
    "\n",
    "# 训练参数\n",
    "epochs = 2\n",
    "N_samples = len(tokenized)            # 注意是 tokenized，不是 raw\n",
    "batch_size = 16\n",
    "# 正确写法（匹配 drop_remainder=True 后的实际批次数）\n",
    "#steps_per_epoch = N_samples // batch_size  # 整除\n",
    "steps_per_epoch = math.ceil(N_samples / batch_size)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    train_iter = iter(tf_train)\n",
    "\n",
    "    if epoch >= 1:\n",
    "        lambda_weight = 1e-1  # 正则项权重，可调\n",
    "\n",
    "    pbar = tqdm(range(steps_per_epoch), desc=\"Training\")\n",
    "    for step in pbar:\n",
    "    #for step in range(steps_per_epoch):\n",
    "        batch = next(train_iter)\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # 外层 Tape：用于最终反向传播\n",
    "        with tf.GradientTape() as outer_tape:\n",
    "            # 内层 Tape：用于计算 loss 的梯度\n",
    "            with tf.GradientTape() as inner_tape:\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels, training=True)\n",
    "                \n",
    "                logits = outputs.logits\n",
    "                #loss = loss_fn(labels, logits)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            # 计算梯度敏感性鲁棒性正则项（所有参数）\n",
    "            grads = inner_tape.gradient(loss, model.trainable_variables)\n",
    "            # 分层归一：每层求和后除以参数数目\n",
    "            robust_penalty = tf.add_n([\n",
    "                tf.reduce_sum(tf.abs(g)) / tf.cast(tf.size(g), tf.float32)\n",
    "                for g in grads if g is not None\n",
    "            ])\n",
    "\n",
    "            robust_loss = lambda_weight * robust_penalty\n",
    "\n",
    "            \n",
    "            # 将 loss.numpy() 转为 float\n",
    "            loss_val   = float(loss)\n",
    "            robust_val = float(robust_loss)\n",
    "            pbar.set_postfix({\n",
    "                \"loss\":   f\"{loss_val:.4f}\",\n",
    "                \"robust_loss\": f\"{robust_val:.4f}\"\n",
    "            })\n",
    "            \n",
    "\n",
    "            total_loss = loss + robust_loss\n",
    "\n",
    "        # 外层梯度计算：对 total_loss 求导并更新\n",
    "        final_grads = outer_tape.gradient(total_loss, model.trainable_variables)\n",
    "        #optimizer.apply_gradients(zip(final_grads, model.trainable_variables))\n",
    "\n",
    "        clipped_grads = tf.clip_by_global_norm(final_grads, 1.0)[0]  # 全局裁剪\n",
    "        optimizer.apply_gradients(zip(clipped_grads, model.trainable_variables))\n",
    "\n",
    "\n",
    "    # 验证阶段（无扰动）\n",
    "    total_val_loss = 0.0\n",
    "    num_val_batches = 0\n",
    "\n",
    "    for batch in tf_test:\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels, training=False)\n",
    "        logits = outputs.logits\n",
    "        val_loss = loss_fn(labels, logits)\n",
    "\n",
    "        total_val_loss += val_loss\n",
    "        num_val_batches += 1\n",
    "\n",
    "    avg_val_loss = total_val_loss / num_val_batches\n",
    "    print(f\"Validation Loss after epoch {epoch + 1}: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8812e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# —— 调用鲁棒性评估 —— \n",
    "sdc_rate, crash_rate, total = evaluate_robustness(\n",
    "    model=model,\n",
    "    tf_test=tf_test,\n",
    "    num_batches=50,            # 或者根据你测试数据量调整\n",
    "    conf_file=\"/Users/lordtarn1shed/TensorFI2/experiments/layer-states/confFiles/sample.yaml\"\n",
    ")\n",
    "print(f\"Epoch {epoch + 1}  >>  SDC Rate: {sdc_rate:.2f}, Crash Rate: {crash_rate:.2f}, Total: {total:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361cc4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推理示例\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "sample = \"translate English to Chinese: This is a test.\"\n",
    "inp = tokenizer(sample, return_tensors=\"tf\", padding=\"max_length\", max_length=64)\n",
    "with tf.device('/CPU:0'):\n",
    "    out = model.generate(inp[\"input_ids\"], attention_mask=inp[\"attention_mask\"], num_beams=NUM_BEAMS)\n",
    "print(\"Translation:\", tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fd6d33-631e-4bb5-ab06-d471d8527a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (transformer)",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
