{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8a51eec-74df-4c53-a8a8-104b5532ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from src import tensorfi2 as tfi\n",
    "\n",
    "# 1. 下载 Tatoeba 英法并只取前 1% 样本以加速实验\n",
    "raw = load_dataset(\"tatoeba\", lang1=\"en\", lang2=\"fr\", split=\"train[:1%]\")  # Tatoeba 英法平行语料 :contentReference[oaicite:4]{index=4}\n",
    "\n",
    "# 2. 初始化分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# 3. 预处理：添加翻译前缀，tokenize，并将 padding token 转为 -100 以忽略\n",
    "def preprocess(examples):\n",
    "    # 1. 添加翻译前缀并编码输入\n",
    "    inputs = [\"translate English to French: \" + ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=64, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # 2. 编码目标文本并生成 decoder_input_ids\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        targets = tokenizer(\n",
    "            [ex[\"fr\"] for ex in examples[\"translation\"]],\n",
    "            max_length=64,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "    \n",
    "    # 3. 生成 decoder_input_ids（右移的 labels）\n",
    "    labels = targets[\"input_ids\"]\n",
    "    decoder_input_ids = [\n",
    "        [tokenizer.pad_token_id] + seq[:-1]  # 在开头添加 pad，并右移\n",
    "        for seq in labels\n",
    "    ]\n",
    "    \n",
    "    # 4. 替换 labels 中的 pad_token_id 为 -100（忽略损失计算）\n",
    "    labels = [\n",
    "        [(t if t != tokenizer.pad_token_id else -100) for t in seq]\n",
    "        for seq in labels\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    model_inputs[\"decoder_input_ids\"] = decoder_input_ids  # 关键修复\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "tokenized = raw.map(preprocess, batched=True, remove_columns=[\"translation\"])  # 动态预处理 :contentReference[oaicite:5]{index=5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58b57ec6-156d-442d-8f7b-fe506609e6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import TFAutoModelForSeq2SeqLM\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')  # GPU自动加速\n",
    "\n",
    "# 可定制的超参数\n",
    "MODEL_CHECKPOINT = \"t5-small\"   # 可换成 t5-base、t5-large 等 :contentReference[oaicite:6]{index=6}\n",
    "LEARNING_RATE     = 5e-5\n",
    "NUM_BEAMS         = 4\n",
    "\n",
    "# 加载模型\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)  # 加载预训练 T5 编码器-解码器 :contentReference[oaicite:7]{index=7}\n",
    "\n",
    "# 若需调整层数 / 头数，可在此处重新定义 config\n",
    "# e.g., model.config.num_layers = 4; model.config.num_heads = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "741dd93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train =( \n",
    "    tokenized.to_tf_dataset(\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"labels\"],\n",
    "        batch_size=16,\n",
    "        shuffle=True\n",
    "    )\n",
    "    .cache()                        # 缓存到内存/磁盘\n",
    "    .shuffle(2000, reshuffle_each_iteration=True)  # 增加buffer_size\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)  # 异步预取\n",
    "    .repeat()  # 避免每个epoch重新初始化\n",
    ")\n",
    "\n",
    "tf_test = tokenized.to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"labels\"],\n",
    "    batch_size=16,\n",
    "    shuffle=False\n",
    ").repeat().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27b05e41-5e9b-4d15-bf7a-539821b8a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bit_flip_attack(model, flip_prob=1e-4):\n",
    "    \"\"\"模拟权重中的随机比特翻转\"\"\"\n",
    "    original_weights = model.get_weights()\n",
    "    corrupted_weights = []\n",
    "    \n",
    "    for w in original_weights:\n",
    "        if len(w.shape) == 0:  # 跳过标量（如某些优化器状态）\n",
    "            corrupted_weights.append(w)\n",
    "            continue\n",
    "        \n",
    "        # 生成随机掩码选择翻转位置\n",
    "        mask = tf.random.uniform(w.shape) < flip_prob\n",
    "        # 生成随机翻转量（±1e-3模拟比特错误）\n",
    "        delta = tf.random.uniform(w.shape, minval=-1e-3, maxval=1e-3) * tf.cast(mask, tf.float32)\n",
    "        corrupted = w + delta\n",
    "        corrupted_weights.append(corrupted.numpy())\n",
    "    \n",
    "    model.set_weights(corrupted_weights)\n",
    "\n",
    "def evaluate_robustness(model, test_dataset, num_samples, flip_prob=1e-4):\n",
    "    test_dataset = test_dataset.repeat()\n",
    "    test_iter = iter(test_dataset)\n",
    "    \n",
    "    # 动态计算实际可评估的最大样本数\n",
    "    total_samples = min(num_samples, len(test_dataset) * test_dataset._batch_size)  # 近似计算\n",
    "    num_batches = total_samples // test_dataset._batch_size\n",
    "    \n",
    "    total_sdc = 0\n",
    "    total_crash = 0\n",
    "    total_tested = 0\n",
    "    original_weights = model.get_weights()\n",
    "\n",
    "    # 1. 生成基准输出（带进度条）\n",
    "    baseline_outputs = []\n",
    "    print(\"\\nGenerating Baseline Predictions...\")\n",
    "    for _ in tqdm(range(num_batches), desc=\"Baseline\"):\n",
    "        batch = next(test_iter)\n",
    "        logits = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            decoder_input_ids=batch[\"decoder_input_ids\"],\n",
    "            training=False\n",
    "        ).logits\n",
    "        baseline_outputs.append(tf.argmax(logits, axis=-1))\n",
    "    \n",
    "    # 2. 注入错误评估（带实时指标显示）\n",
    "    print(\"\\nEvaluating Robustness with Bit Flips...\")\n",
    "    test_iter = iter(test_dataset)  # 重置迭代器\n",
    "    pbar = tqdm(total=total_samples, desc=\"Testing\", unit=\"sample\")\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        batch = next(test_iter)\n",
    "        batch_size = batch[\"input_ids\"].shape[0]\n",
    "        \n",
    "        try:\n",
    "            bit_flip_attack(model, flip_prob)\n",
    "            \n",
    "            logits = model(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                decoder_input_ids=batch[\"decoder_input_ids\"],\n",
    "                training=False\n",
    "            ).logits\n",
    "            preds = tf.argmax(logits, axis=-1)\n",
    "            \n",
    "            # 计算当前批次的 SDC\n",
    "            sdc = tf.reduce_sum(tf.cast(preds != baseline_outputs[i], tf.int32)).numpy()\n",
    "            total_sdc += sdc\n",
    "            \n",
    "        except Exception as e:\n",
    "            total_crash += batch_size\n",
    "            \n",
    "        finally:\n",
    "            model.set_weights(original_weights)\n",
    "            total_tested += batch_size\n",
    "            \n",
    "            # 更新进度条描述\n",
    "            current_sdc = total_sdc / total_tested if total_tested > 0 else 0\n",
    "            current_crash = total_crash / total_tested if total_tested > 0 else 0\n",
    "            pbar.set_postfix({\n",
    "                \"SDC%\": f\"{current_sdc*100:.2f}%\",\n",
    "                \"Crash%\": f\"{current_crash*100:.2f}%\"\n",
    "            })\n",
    "            pbar.update(batch_size)  # 按实际批次大小更新进度\n",
    "            \n",
    "    pbar.close()\n",
    "    \n",
    "    return total_sdc / total_tested, total_crash / total_tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71eb64ae-9150-485b-9900-d942eab3c639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 166/166 [07:13<00:00,  2.61s/it, loss=0.8035, robust_loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0y/bfnk7_gj5s91nr8nhk4544fc0000gn/T/ipykernel_27204/4033256621.py:93: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  loss_value = float(outputs.loss.numpy())  # 或 outputs.loss.numpy().item()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4813\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The dataset is infinite.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 99\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_val_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# 现在 avg_val_loss 是 float\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# 新增鲁棒性评估\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m sdc_rate, crash_rate \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_robustness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflip_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSDC Rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msdc_rate\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Crash Rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcrash_rate\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 25\u001b[0m, in \u001b[0;36mevaluate_robustness\u001b[0;34m(model, test_dataset, num_samples, flip_prob)\u001b[0m\n\u001b[1;32m     22\u001b[0m test_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(test_dataset)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 动态计算实际可评估的最大样本数\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(num_samples, \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m test_dataset\u001b[38;5;241m.\u001b[39m_batch_size)  \u001b[38;5;66;03m# 近似计算\u001b[39;00m\n\u001b[1;32m     26\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m total_samples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m test_dataset\u001b[38;5;241m.\u001b[39m_batch_size\n\u001b[1;32m     28\u001b[0m total_sdc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/transformer/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:531\u001b[0m, in \u001b[0;36mDatasetV2.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    529\u001b[0m length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcardinality()\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m==\u001b[39m INFINITE:\n\u001b[0;32m--> 531\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe dataset is infinite.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m==\u001b[39m UNKNOWN:\n\u001b[1;32m    533\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe dataset length is unknown.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: The dataset is infinite."
     ]
    }
   ],
   "source": [
    "# 定义 loss 函数和优化器\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "lambda_weight = 0  # 正则项权重，可调\n",
    "\n",
    "# 训练参数\n",
    "epochs = 2\n",
    "N_samples = len(tokenized)            # 注意是 tokenized，不是 raw\n",
    "batch_size = 16\n",
    "# 正确写法（匹配 drop_remainder=True 后的实际批次数）\n",
    "#steps_per_epoch = N_samples // batch_size  # 整除\n",
    "steps_per_epoch = math.ceil(N_samples / batch_size)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    train_iter = iter(tf_train)\n",
    "\n",
    "    if epoch >= 1:\n",
    "        lambda_weight = 1e-1  # 正则项权重，可调\n",
    "\n",
    "    pbar = tqdm(range(steps_per_epoch), desc=\"Training\")\n",
    "    for step in pbar:\n",
    "    #for step in range(steps_per_epoch):\n",
    "        batch = next(train_iter)\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # 外层 Tape：用于最终反向传播\n",
    "        with tf.GradientTape() as outer_tape:\n",
    "            # 内层 Tape：用于计算 loss 的梯度\n",
    "            with tf.GradientTape() as inner_tape:\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, decoder_input_ids=batch[\"decoder_input_ids\"], labels=labels, training=True)\n",
    "                \n",
    "                logits = outputs.logits\n",
    "                #loss = loss_fn(labels, logits)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            # 计算梯度敏感性鲁棒性正则项（所有参数）\n",
    "            grads = inner_tape.gradient(loss, model.trainable_variables)\n",
    "            # 分层归一：每层求和后除以参数数目\n",
    "            robust_penalty = tf.add_n([\n",
    "                tf.reduce_sum(tf.abs(g)) / tf.cast(tf.size(g), tf.float32)\n",
    "                for g in grads if g is not None\n",
    "            ])\n",
    "\n",
    "            robust_loss = lambda_weight * robust_penalty\n",
    "\n",
    "            \n",
    "            # 将 loss.numpy() 转为 float\n",
    "            loss_val   = float(loss)\n",
    "            robust_val = float(robust_loss)\n",
    "            pbar.set_postfix({\n",
    "                \"loss\":   f\"{loss_val:.4f}\",\n",
    "                \"robust_loss\": f\"{robust_val:.4f}\"\n",
    "            })\n",
    "            \n",
    "\n",
    "            total_loss = loss + robust_loss\n",
    "\n",
    "        # 外层梯度计算：对 total_loss 求导并更新\n",
    "        final_grads = outer_tape.gradient(total_loss, model.trainable_variables)\n",
    "        #optimizer.apply_gradients(zip(final_grads, model.trainable_variables))\n",
    "\n",
    "        clipped_grads = tf.clip_by_global_norm(final_grads, 1.0)[0]  # 全局裁剪\n",
    "        optimizer.apply_gradients(zip(clipped_grads, model.trainable_variables))\n",
    "\n",
    "    print(f\"epoch {epoch + 1} complete\")\n",
    "\n",
    "\n",
    "    val_steps = math.ceil(len(tokenized) / batch_size)\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    for i, batch in zip(range(val_steps), tf_test):\n",
    "        # 提取输入参数\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        decoder_input_ids = batch[\"decoder_input_ids\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            labels=labels,\n",
    "            training=False\n",
    "        )\n",
    "        \n",
    "        # 关键修复：确保损失为标量\n",
    "        loss_value = float(outputs.loss.numpy())  # 或 outputs.loss.numpy().item()\n",
    "        total_val_loss += loss_value\n",
    "\n",
    "    avg_val_loss = total_val_loss / val_steps\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")  # 现在 avg_val_loss 是 float\n",
    "    # 新增鲁棒性评估\n",
    "    sdc_rate, crash_rate = evaluate_robustness(model, tf_test, num_samples=100, flip_prob=1e-4)\n",
    "    print(f\"SDC Rate: {sdc_rate:.4f}, Crash Rate: {crash_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1516bc39-bcef-4713-bc72-78b726e4c48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推理示例\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "sample = \"translate English to French: This is a test.\"\n",
    "inp = tokenizer(sample, return_tensors=\"tf\", padding=\"max_length\", max_length=64)\n",
    "with tf.device('/CPU:0'):\n",
    "    out = model.generate(inp[\"input_ids\"], attention_mask=inp[\"attention_mask\"], num_beams=NUM_BEAMS)\n",
    "print(\"Translation:\", tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3f9168-b564-4798-b476-8a4c904e44f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注入硬件错误\n",
    "tfi.inject(model=model,\n",
    "           confFile=\"/Users/lordtarn1shed/TensorFI2/experiments/layer-states/confFiles/sample.yaml\",\n",
    "           log_level=\"DEBUG\")\n",
    "\n",
    "\n",
    "# 评估故障\n",
    "total_val_loss = 0.0\n",
    "num_val_batches = 0\n",
    "\n",
    "for batch in tf_test:\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels, training=False)\n",
    "    logits = outputs.logits\n",
    "    val_loss = loss_fn(labels, logits)\n",
    "\n",
    "    total_val_loss += val_loss\n",
    "    num_val_batches += 1\n",
    "\n",
    "avg_val_loss = total_val_loss / num_val_batches\n",
    "print(f\"Validation Loss after epoch {epoch + 1}: {avg_val_loss:.4f}\")\n",
    "\n",
    "sdc_rate, crash_rate = evaluate_robustness(model, tf_test, num_samples=100, flip_prob=1e-4)\n",
    "print(f\"SDC Rate: {sdc_rate:.4f}, Crash Rate: {crash_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dca03d-3798-4638-b92c-41e798ff0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推理示例\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "sample = \"translate English to French: This is a test.\"\n",
    "inp = tokenizer(sample, return_tensors=\"tf\", padding=\"max_length\", max_length=64)\n",
    "with tf.device('/CPU:0'):\n",
    "    out = model.generate(inp[\"input_ids\"], attention_mask=inp[\"attention_mask\"], num_beams=NUM_BEAMS)\n",
    "print(\"Translation:\", tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (transformer)",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
